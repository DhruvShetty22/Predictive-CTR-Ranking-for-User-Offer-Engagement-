{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38e9ffbe-6d4f-4e5b-a8cf-a8088ba6905e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Customer Fallback Clusters: 50\n",
      "üìà Global mean vector shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load direct customer embeddings\n",
    "id2_to_vec = joblib.load(\"../TransEncoding/customer_id_to_embedding.pkl\")\n",
    "\n",
    "# Load fallback strategy components from single file\n",
    "fallbacks = joblib.load(\"../TransEncoding/customer_fallbacks.pkl\")\n",
    "customer_centroids = fallbacks[\"centroids\"]\n",
    "global_mean_vector = fallbacks[\"global_mean\"]\n",
    "\n",
    "# Optional: check fallback stats\n",
    "print(f\"üìä Customer Fallback Clusters: {customer_centroids.shape[0]}\")\n",
    "print(f\"üìà Global mean vector shape: {global_mean_vector.shape}\")\n",
    "\n",
    "# Embedding resolver function\n",
    "def get_customer_embedding(id2, id2_to_vec, centroids, global_mean):\n",
    "    \"\"\"\n",
    "    Hybrid strategy to resolve customer embedding.\n",
    "\n",
    "    1. If ID found, use direct embedding.\n",
    "    2. Else, fall back to random cluster center.\n",
    "    3. Else, fall back to global mean.\n",
    "    \"\"\"\n",
    "    if id2 in id2_to_vec:\n",
    "        return id2_to_vec[id2]\n",
    "    elif len(centroids) > 0:\n",
    "        cluster_idx = np.random.randint(len(centroids))\n",
    "        return centroids[cluster_idx]\n",
    "    else:\n",
    "        return global_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b08682-eaf6-4b1b-ab61-f1fc1a3a65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Offer fallback clusters: 10\n",
      "üìà Global mean offer vector shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# -------- Load Offer Embedding + Fallbacks -------- #\n",
    "id3_to_vec = joblib.load(\"../OfferEncoding/offer_id_to_embedding.pkl\")\n",
    "offer_centroids = joblib.load(\"../OfferEncoding/offer_cluster_centers.pkl\")\n",
    "global_offer_mean = np.load(\"../OfferEncoding/global_mean_offer_vector.npy\")\n",
    "\n",
    "print(f\"üìä Offer fallback clusters: {offer_centroids.shape[0]}\")\n",
    "print(f\"üìà Global mean offer vector shape: {global_offer_mean.shape}\")\n",
    "\n",
    "# -------- Resolver Function -------- #\n",
    "def get_offer_embedding(id3, id3_to_vec, centroids, global_mean):\n",
    "    \"\"\"\n",
    "    Hybrid strategy to resolve offer embedding.\n",
    "\n",
    "    1. If ID found, use direct embedding.\n",
    "    2. Else, use random cluster center.\n",
    "    3. Else, use global mean.\n",
    "    \"\"\"\n",
    "    if id3 in id3_to_vec:\n",
    "        return id3_to_vec[id3]\n",
    "    elif len(centroids) > 0:\n",
    "        cluster_idx = np.random.randint(len(centroids))\n",
    "        return centroids[cluster_idx]\n",
    "    else:\n",
    "        return global_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0c15df1-4b40-427d-b7e5-52c61f162578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load offer hybrid data\n",
    "offer_id_to_vec = joblib.load(\"../OfferEncoding/offer_id_to_embedding.pkl\")\n",
    "offer_clusters = joblib.load(\"../OfferEncoding/offer_cluster_centers.pkl\")\n",
    "global_offer_vec = np.load(\"../OfferEncoding/global_mean_offer_vector.npy\")\n",
    "\n",
    "# Load customer hybrid data\n",
    "customer_fallback = joblib.load(\"../TransEncoding/customer_fallbacks.pkl\")\n",
    "customer_id_to_vec = joblib.load(\"../TransEncoding/customer_id_to_embedding.pkl\")\n",
    "customer_centroids = customer_fallback[\"centroids\"]\n",
    "global_customer_vec = customer_fallback[\"global_mean\"]\n",
    "\n",
    "# Load event data\n",
    "event_pair = pd.read_parquet(\"../EventsEncoding/event_pair_agg.parquet\").set_index(['id2', 'id3'])\n",
    "event_customer = pd.read_parquet(\"../EventsEncoding/event_customer_agg.parquet\").set_index('id2')\n",
    "event_offer = pd.read_parquet(\"../EventsEncoding/event_offer_agg.parquet\").set_index('id3')\n",
    "event_cols = ['clicks', 'views', 'click_rate', 'avg_click_delay',\n",
    "              'min_click_delay', 'max_click_delay', 'std_click_delay']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7482e944-5bd2-4769-926d-496f44eedb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def closest_vector(centroids, global_vec):\n",
    "    dists = np.linalg.norm(centroids - global_vec, axis=1)\n",
    "    return centroids[np.argmin(dists)]\n",
    "\n",
    "def get_customer_embedding(id2, id2_to_vec, centroids, global_mean):\n",
    "    vec = id2_to_vec.get(id2)\n",
    "    if vec is not None:\n",
    "        return vec\n",
    "    else:\n",
    "        return closest_vector(centroids, global_mean)\n",
    "\n",
    "def build_features(data_path, is_train=True):\n",
    "    print(f\"\\nüì¶ Loading {'train' if is_train else 'test'} data\")\n",
    "    df = pd.read_parquet(data_path)\n",
    "    ids = df[['id1', 'id2', 'id3', 'id5']].copy()\n",
    "\n",
    "    df[\"id2\"] = df[\"id2\"].astype(str)\n",
    "    df[\"id3\"] = df[\"id3\"].astype(str)\n",
    "\n",
    "    if is_train:\n",
    "        if 'y' not in df.columns:\n",
    "            raise ValueError(\"Train data must contain a 'y' column.\")\n",
    "        labels = df['y'].values\n",
    "\n",
    "    # ----------------- Load Offer Hybrid Embeddings ------------------ #\n",
    "    offer_id_to_vec = joblib.load(\"../OfferEncoding/offer_id_to_embedding.pkl\")\n",
    "    offer_clusters = joblib.load(\"../OfferEncoding/offer_cluster_centers.pkl\")\n",
    "    global_offer_vec = np.load(\"../OfferEncoding/global_mean_offer_vector.npy\")\n",
    "    offer_vec_dim = global_offer_vec.shape[0]\n",
    "\n",
    "    # ----------------- Load Customer Hybrid Embeddings ------------------ #\n",
    "    customer_id_to_vec = joblib.load(\"../TransEncoding/customer_id_to_embedding.pkl\")\n",
    "    customer_fallback = joblib.load(\"../TransEncoding/customer_fallbacks.pkl\")\n",
    "    customer_centroids = customer_fallback[\"centroids\"]\n",
    "    global_customer_vec = customer_fallback[\"global_mean\"]\n",
    "    behavior_dim = global_customer_vec.shape[0]\n",
    "\n",
    "    # ----------------- Load Event Aggregates ------------------ #\n",
    "    event_pair = pd.read_parquet(\"../EventsEncoding/event_pair_agg.parquet\").set_index(['id2', 'id3'])\n",
    "    event_customer = pd.read_parquet(\"../EventsEncoding/event_customer_agg.parquet\").set_index('id2')\n",
    "    event_offer = pd.read_parquet(\"../EventsEncoding/event_offer_agg.parquet\").set_index('id3')\n",
    "    event_cols = ['clicks', 'views', 'click_rate', 'avg_click_delay',\n",
    "                  'min_click_delay', 'max_click_delay', 'std_click_delay']\n",
    "\n",
    "    # ----------------- Fix f-columns ------------------ #\n",
    "    all_f_cols = [f\"f{i}\" for i in range(1, 367)]\n",
    "    f_categorical_cols = []\n",
    "    f_numeric_cols = []\n",
    "\n",
    "    for col in all_f_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "                f_numeric_cols.append(col)\n",
    "            except:\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col].astype(str))\n",
    "                f_categorical_cols.append(col)\n",
    "        else:\n",
    "            f_numeric_cols.append(col)\n",
    "\n",
    "    numeric_f_cols = f_numeric_cols + f_categorical_cols\n",
    "    df[numeric_f_cols] = df[numeric_f_cols].fillna(0)\n",
    "\n",
    "    print(f\"‚úÖ Using {len(numeric_f_cols)} total f-features ({len(f_numeric_cols)} numeric + {len(f_categorical_cols)} encoded categorical)\")\n",
    "\n",
    "    # ----------------- Build Feature Matrix ------------------ #\n",
    "    features = []\n",
    "    missing_offer = 0\n",
    "    missing_event_pair = 0\n",
    "    missing_event_customer = 0\n",
    "    missing_event_offer = 0\n",
    "    missing_behavior = 0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        base_feats = row[numeric_f_cols].astype(np.float32).values\n",
    "\n",
    "        # --- OFFER EMBEDDING (Hybrid) ---\n",
    "        offer_vec = offer_id_to_vec.get(row[\"id3\"])\n",
    "        if offer_vec is None:\n",
    "            offer_vec = closest_vector(offer_clusters, global_offer_vec)\n",
    "            missing_offer += 1\n",
    "\n",
    "        # --- EVENT EMBEDDING (3-level fallback) ---\n",
    "        key_pair = (row[\"id2\"], row[\"id3\"])\n",
    "        if key_pair in event_pair.index:\n",
    "            event_vec = event_pair.loc[key_pair].values\n",
    "        elif row[\"id2\"] in event_customer.index:\n",
    "            event_vec = event_customer.loc[row[\"id2\"]].values\n",
    "            missing_event_pair += 1\n",
    "        elif row[\"id3\"] in event_offer.index:\n",
    "            event_vec = event_offer.loc[row[\"id3\"]].values\n",
    "            missing_event_pair += 1\n",
    "            missing_event_customer += 1\n",
    "        else:\n",
    "            event_vec = np.zeros(len(event_cols))\n",
    "            missing_event_pair += 1\n",
    "            missing_event_customer += 1\n",
    "            missing_event_offer += 1\n",
    "\n",
    "        # --- CUSTOMER EMBEDDING (Hybrid) ---\n",
    "        behavior_vec = get_customer_embedding(row[\"id2\"], customer_id_to_vec, customer_centroids, global_customer_vec)\n",
    "        if row[\"id2\"] not in customer_id_to_vec:\n",
    "            missing_behavior += 1\n",
    "\n",
    "        # Combine\n",
    "        features.append(np.concatenate([base_feats, offer_vec, event_vec, behavior_vec]))\n",
    "\n",
    "    X = np.vstack(features)\n",
    "\n",
    "    print(f\"‚ö†Ô∏è Missing offers: {missing_offer}\")\n",
    "    print(f\"‚ö†Ô∏è Missing event pair: {missing_event_pair}\")\n",
    "    print(f\"‚Ü™Ô∏è Fallback to customer-level: {missing_event_customer}\")\n",
    "    print(f\"‚Ü™Ô∏è Fallback to offer-level: {missing_event_offer}\")\n",
    "    print(f\"‚ö†Ô∏è Missing customer behavior: {missing_behavior}\")\n",
    "    print(f\"‚úÖ Final feature matrix shape: {X.shape}\")\n",
    "\n",
    "    if is_train:\n",
    "        return X, labels, ids.values\n",
    "    else:\n",
    "        return X, ids.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a823085-efad-4422-809a-c54ac8c6f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# ----------------------- Training Pipeline ----------------------- #\n",
    "def train_model(train_path=\"../../Dataset/train_data.parquet\"):\n",
    "    print(\"üöÄ Starting training...\")\n",
    "\n",
    "    # Step 1: Build Features\n",
    "    X, y, ids = build_features(train_path, is_train=True)\n",
    "\n",
    "    # Step 2: Split Data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"üß™ Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "    # Step 3: Define LGBMClassifier\n",
    "    from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=64,\n",
    "        max_depth=-1,\n",
    "        boosting_type='gbdt',\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample_freq=5,\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=25),\n",
    "            log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Step 5: Evaluate\n",
    "    y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_val_pred)\n",
    "    ll = log_loss(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"\\n‚úÖ Validation AUC: {auc:.4f}\")\n",
    "    print(f\"üìâ Validation LogLoss: {ll:.4f}\")\n",
    "\n",
    "    # Step 6: Save artifacts\n",
    "    joblib.dump(model, \"lightgbm_model.pkl\")\n",
    "    np.save(\"val_preds.npy\", y_val_pred)\n",
    "    print(\"üíæ Saved: model & validation predictions.\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ef486a2-b5e1-4b33-ac8a-ae53db72be8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "\n",
      "üì¶ Loading train data\n",
      "‚úÖ Using 366 total f-features (357 numeric + 9 encoded categorical)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 770164/770164 [14:39<00:00, 875.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Missing offers: 100\n",
      "‚ö†Ô∏è Missing event pair: 770164\n",
      "‚Ü™Ô∏è Fallback to customer-level: 770164\n",
      "‚Ü™Ô∏è Fallback to offer-level: 0\n",
      "‚ö†Ô∏è Missing customer behavior: 770164\n",
      "‚úÖ Final feature matrix shape: (770164, 405)\n",
      "üß™ Training samples: 616131, Validation samples: 154033\n",
      "[LightGBM] [Info] Number of positive: 29641, number of negative: 586490\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.031562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 52719\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 323\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048108 -> initscore=-2.984997\n",
      "[LightGBM] [Info] Start training from score -2.984997\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[50]\tvalid_0's auc: 0.936016\tvalid_0's binary_logloss: 0.0899051\n",
      "[100]\tvalid_0's auc: 0.944888\tvalid_0's binary_logloss: 0.0820331\n",
      "[150]\tvalid_0's auc: 0.948765\tvalid_0's binary_logloss: 0.0788589\n",
      "[200]\tvalid_0's auc: 0.950419\tvalid_0's binary_logloss: 0.0773251\n",
      "[250]\tvalid_0's auc: 0.951211\tvalid_0's binary_logloss: 0.0764583\n",
      "[300]\tvalid_0's auc: 0.951664\tvalid_0's binary_logloss: 0.0758387\n",
      "[350]\tvalid_0's auc: 0.952312\tvalid_0's binary_logloss: 0.0751688\n",
      "[400]\tvalid_0's auc: 0.952724\tvalid_0's binary_logloss: 0.0747635\n",
      "[450]\tvalid_0's auc: 0.952993\tvalid_0's binary_logloss: 0.0743546\n",
      "[500]\tvalid_0's auc: 0.953343\tvalid_0's binary_logloss: 0.0740711\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[495]\tvalid_0's auc: 0.953372\tvalid_0's binary_logloss: 0.0740677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Validation AUC: 0.9534\n",
      "üìâ Validation LogLoss: 0.0741\n",
      "üíæ Saved: model & validation predictions.\n"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "698f9c9a-447f-4b12-940a-d9f6f58864fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_and_save(\n",
    "    test_path=\"../../Dataset/test_data.parquet\",\n",
    "    model_path=\"lightgbm_model.pkl\",\n",
    "    output_dir=r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\AMEX\\Code\\Model\"\n",
    "):\n",
    "    print(\"üîÆ Generating predictions on test set...\")\n",
    "\n",
    "    # 1. Load features from test set\n",
    "    X_test, id_matrix = build_features(test_path, is_train=False)\n",
    "\n",
    "    # 2. Load the trained model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # 3. Predict probabilities\n",
    "    y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 4. Save raw prediction + IDs\n",
    "    np.save(f\"{output_dir}/test_preds.npy\", y_test_pred)\n",
    "    np.save(f\"{output_dir}/test_ids.npy\", id_matrix)\n",
    "    print(f\"‚úÖ Saved raw predictions to {output_dir}/test_preds.npy\")\n",
    "\n",
    "    # 5. Save CSV for submission\n",
    "    sub_df = pd.DataFrame({\n",
    "        \"id1\": id_matrix[:, 0],\n",
    "        \"id2\": id_matrix[:, 1],\n",
    "        \"id3\": id_matrix[:, 2],\n",
    "        \"id5\": id_matrix[:, 3],\n",
    "        \"prediction\": y_test_pred\n",
    "    })\n",
    "    sub_df.to_csv(f\"{output_dir}/submission.csv\", index=False)\n",
    "    print(f\"üì§ Submission CSV saved to {output_dir}/submission.csv\")\n",
    "\n",
    "    return sub_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cacdb019-a393-43ef-aead-ce167e5d6fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Generating predictions on test set...\n",
      "\n",
      "üì¶ Loading test data\n",
      "‚úÖ Using 366 total f-features (357 numeric + 9 encoded categorical)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 369301/369301 [04:56<00:00, 1244.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Missing offers: 53\n",
      "‚ö†Ô∏è Missing event pair: 369301\n",
      "‚Ü™Ô∏è Fallback to customer-level: 369301\n",
      "‚Ü™Ô∏è Fallback to offer-level: 2\n",
      "‚ö†Ô∏è Missing customer behavior: 369301\n",
      "‚úÖ Final feature matrix shape: (369301, 405)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved raw predictions to C:\\Users\\dhruv\\OneDrive\\Desktop\\AMEX\\Code\\Model/test_preds.npy\n",
      "üì§ Submission CSV saved to C:\\Users\\dhruv\\OneDrive\\Desktop\\AMEX\\Code\\Model/submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>id3</th>\n",
       "      <th>id5</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1362907_91950_16-23_2023-11-04 18:56:26.000794</td>\n",
       "      <td>1362907</td>\n",
       "      <td>91950</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>0.002842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1082599_88356_16-23_2023-11-04 06:08:53.373</td>\n",
       "      <td>1082599</td>\n",
       "      <td>88356</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>0.032592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1888466_958700_16-23_2023-11-05 10:07:28.000725</td>\n",
       "      <td>1888466</td>\n",
       "      <td>958700</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>0.965038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1888971_795739_16-23_2023-11-04 12:25:28.244</td>\n",
       "      <td>1888971</td>\n",
       "      <td>795739</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>0.003186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1256369_82296_16-23_2023-11-05 06:45:26.657</td>\n",
       "      <td>1256369</td>\n",
       "      <td>82296</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>0.003625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369296</th>\n",
       "      <td>1874443_95537_16-23_2023-11-05 09:21:24.182</td>\n",
       "      <td>1874443</td>\n",
       "      <td>95537</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>0.014164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369297</th>\n",
       "      <td>1541978_5718_16-23_2023-11-05 00:56:43.946</td>\n",
       "      <td>1541978</td>\n",
       "      <td>5718</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>0.004318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369298</th>\n",
       "      <td>1887841_85905_16-23_2023-11-05 20:40:43.312</td>\n",
       "      <td>1887841</td>\n",
       "      <td>85905</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>0.015275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369299</th>\n",
       "      <td>1569367_944713_16-23_2023-11-05 00:43:04.335</td>\n",
       "      <td>1569367</td>\n",
       "      <td>944713</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>0.022691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369300</th>\n",
       "      <td>1086547_60142_16-23_2023-11-05 10:37:36.747</td>\n",
       "      <td>1086547</td>\n",
       "      <td>60142</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>0.041436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369301 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id1      id2     id3  \\\n",
       "0        1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n",
       "1           1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n",
       "2       1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n",
       "3          1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n",
       "4           1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n",
       "...                                                 ...      ...     ...   \n",
       "369296      1874443_95537_16-23_2023-11-05 09:21:24.182  1874443   95537   \n",
       "369297       1541978_5718_16-23_2023-11-05 00:56:43.946  1541978    5718   \n",
       "369298      1887841_85905_16-23_2023-11-05 20:40:43.312  1887841   85905   \n",
       "369299     1569367_944713_16-23_2023-11-05 00:43:04.335  1569367  944713   \n",
       "369300      1086547_60142_16-23_2023-11-05 10:37:36.747  1086547   60142   \n",
       "\n",
       "               id5  prediction  \n",
       "0       2023-11-04    0.002842  \n",
       "1       2023-11-04    0.032592  \n",
       "2       2023-11-05    0.965038  \n",
       "3       2023-11-04    0.003186  \n",
       "4       2023-11-05    0.003625  \n",
       "...            ...         ...  \n",
       "369296  2023-11-05    0.014164  \n",
       "369297  2023-11-05    0.004318  \n",
       "369298  2023-11-05    0.015275  \n",
       "369299  2023-11-05    0.022691  \n",
       "369300  2023-11-05    0.041436  \n",
       "\n",
       "[369301 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d0c6d-e3f5-44c7-a3bd-ede8fd37c1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83edcccf-b963-4bd0-b07c-6a5b17629914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "model = joblib.load(\"../../Code/FinalModel/lightgbm_model.pkl\")\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=20, importance_type='gain')\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d94d61-1309-4819-8dac-7e1f10be986a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448d8101-409a-41d7-aaa6-a071db09ee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\AMEX\\\\Code\\\\FinalModel'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba98a6-70ff-4bf9-a3fd-96583e7c7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. \"Unknown\" Token During Encoder Training\n",
    "If you're training an embedding model (e.g., an MLP), you can include a dummy \"__UNKNOWN__\" sample during training. That way, the model learns how to represent and handle unseen IDs.\n",
    "\n",
    "\n",
    "3. Similarity-Based Approximation (Advanced)\n",
    "You can:\n",
    "\n",
    "Cluster embeddings\n",
    "\n",
    "Assign new IDs to the nearest cluster center\n",
    "This is more complex and works well when you expect lots of cold-starts.\n",
    "\n",
    "3. Optional: Use FAISS for High-Speed Approximate Search (Advanced)\n",
    "If cluster centers aren't enough and you need fine-grained fallback:\n",
    "\n",
    "Use Facebook‚Äôs FAISS library for fast similarity search.\n",
    "\n",
    "Handles millions of embeddings with sub-second latency.\n",
    "\n",
    "But start with KMeans ‚Üí scale later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7c633-81e4-449b-81e2-ab565a43872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "‚úÖ 3. Hybrid Fallbacks (Best in Practice)\n",
    "The best production systems usually combine:\n",
    "\n",
    "Order\tMethod\tFallback To\n",
    "1\tDirect match\tUse if id2 seen\n",
    "2\tRegression model\tPredict if metadata seen\n",
    "3\tCluster similarity\tNearest center\n",
    "4\tGlobal avg vector\tUse final dummy vec\n",
    "\n",
    "This reduces blind zero vectors to almost zero and makes the model very stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60b2c492-d0de-483f-9657-4c97fe3b9573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\OneDrive\\Desktop\\AMEX\\Code\\Model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
